{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iKQrHajcdB5U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From s:\\Thesis_Work\\tff\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfv_fQ0OdB5b",
        "outputId": "272f3c22-3bab-47b6-b969-d9001830a267"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['hello', 'how are you', 'goodbye'],\n",
              " ['ohe <eos>', 'kemon acho tumi <eos>', 'biday <eos>'],\n",
              " ['<sos> ohe', '<sos> kemon acho tumi', '<sos> biday'])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = ['hello', 'how are you', 'goodbye']\n",
        "output = ['ohe', 'kemon acho tumi', 'biday']\n",
        "\n",
        "outputs=['{} <eos>'.format(x) for x in output ]\n",
        "outputs_i=['<sos> {}'.format(x) for x in output ]\n",
        "inputs,outputs,outputs_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwC26Gp1dB5c",
        "outputId": "bc2aa9f8-7ec2-47a9-8451-d94d85ec7397"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['ohe <eos>', 'kemon acho tumi <eos>', 'biday <eos>'],\n",
              " ['<sos> ohe', '<sos> kemon acho tumi', '<sos> biday'])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs,outputs_i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7SUXG2EOdB5d"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGX5JFcndB5d",
        "outputId": "73c7a795-6166-49c6-ff8e-ed7e8c15c838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique words in input: 5\n",
            "Length of longest sentence in input: 3\n",
            "Total unique words in output: 7\n",
            "Length of longest sentence in output: 5\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "MAX_NUM_WORDS=8\n",
        "\n",
        "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "input_tokenizer.fit_on_texts(inputs)\n",
        "\n",
        "\n",
        "#\n",
        "inputs_seq = input_tokenizer.texts_to_sequences(inputs)\n",
        "#\n",
        "\n",
        "#\n",
        "inputs_word2index = input_tokenizer.word_index\n",
        "\n",
        "# {a:1,go:40,here:100...........}\n",
        "\n",
        "print('Total unique words in input:', len(inputs_word2index))\n",
        "#\n",
        "\n",
        "\n",
        "inputs_numwords = len(inputs_word2index)+1\n",
        "#\n",
        "inputs_maxlen = max(len(s) for s in inputs_seq)\n",
        "print('Length of longest sentence in input:', inputs_maxlen)\n",
        "\n",
        "\n",
        "# related to output!!!!!!!!!!!!>>>>>>>>>>>>>>>>>>\n",
        "\n",
        "\n",
        "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "output_tokenizer.fit_on_texts(outputs_i + outputs)\n",
        "\n",
        "outputs_i_seq = output_tokenizer.texts_to_sequences(outputs_i)\n",
        "outputs_seq = output_tokenizer.texts_to_sequences(outputs)\n",
        "\n",
        "\n",
        "\n",
        "outputs_word2index = output_tokenizer.word_index\n",
        "print('Total unique words in output:', len(outputs_word2index))\n",
        "\n",
        "outputs_numwords = len(outputs_word2index)+1\n",
        "\n",
        "outputs_maxlen = max(len(s) for s in outputs_seq)+1\n",
        "print('Length of longest sentence in output:', outputs_maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3Mk-RxLdB5e",
        "outputId": "8dd33a4e-312f-4664-dc1b-9e4a9a31d489"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'<sos>': 1,\n",
              " '<eos>': 2,\n",
              " 'ohe': 3,\n",
              " 'kemon': 4,\n",
              " 'acho': 5,\n",
              " 'tumi': 6,\n",
              " 'biday': 7}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs_word2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FHA2_pejdB5e"
      },
      "outputs": [],
      "source": [
        "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
        "output_tokenizer.fit_on_texts(outputs_i + outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IHIFDS-7dB5f"
      },
      "outputs": [],
      "source": [
        "Oidx2wrd=output_tokenizer.index_word\n",
        "Owrd2idx={v:k for k,v in Oidx2wrd.items()}\n",
        "\n",
        "# Iidx2wrd=input_tokenizer.index_word\n",
        "# Iwrd2idx={v:k for k,v in Iidx2wrd.items()}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZp5a6I-dB5f",
        "outputId": "b71df374-fa28-4a71-f17a-1eb0b51f5915"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hello': 1, 'how': 2, 'are': 3, 'you': 4, 'goodbye': 5}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0KicbjPdB5g",
        "outputId": "775671f7-e750-444f-82aa-78ba15edcf28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder_input_sequences shape: (3, 3)\n",
            "decoder_inputs_sequences shape: (3, 5)\n",
            "decoder_output_sequences shape: (3, 5)\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "encoder_input_sequences = pad_sequences(inputs_seq, maxlen=inputs_maxlen)\n",
        "print('encoder_input_sequences shape:', encoder_input_sequences.shape)\n",
        "\n",
        "decoder_input_sequences = pad_sequences(outputs_i_seq, maxlen=outputs_maxlen, padding='post')\n",
        "print('decoder_inputs_sequences shape:', decoder_input_sequences.shape)\n",
        "\n",
        "decoder_output_sequences = pad_sequences(outputs_seq, maxlen=outputs_maxlen, padding='post')\n",
        "print('decoder_output_sequences shape:', decoder_output_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvssV0pBdB5g",
        "outputId": "5012e392-2d57-4f0b-b5b4-a50345d06944"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 1],\n",
              "       [2, 3, 4],\n",
              "       [0, 0, 5]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoder_input_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRuPBXwEdB5g",
        "outputId": "e35c96d8-0d53-4b9c-e2f7-5b693ae82a68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 3, 0, 0, 0],\n",
              "       [1, 4, 5, 6, 0],\n",
              "       [1, 7, 0, 0, 0]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoder_input_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hJF-BDZdB5h"
      },
      "source": [
        "## the_model_cometh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LpKnNHpGdB5i"
      },
      "outputs": [],
      "source": [
        "NUM_SENTENCES = 4 # Use only the first 20,000 records.\n",
        "MAX_NUM_WORDS = 4 # Use 20,000 words for tokenizing\n",
        "MAX_SENT_LEN = 6\n",
        "\n",
        "EMBEDDING_SIZE = 5\n",
        "\n",
        "LSTM_NEURONS = 2\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "a4RPbT7UdB5i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From s:\\Thesis_Work\\tff\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "encoder_embedding_layer = Embedding(\n",
        "    inputs_numwords, #super important\n",
        "    EMBEDDING_SIZE\n",
        "    )\n",
        "\n",
        "decoder_embedding_layer = Embedding(\n",
        "    outputs_numwords, #super important\n",
        "    EMBEDDING_SIZE\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "87TWXIugdB5j"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = Input(shape=(inputs_maxlen,))\n",
        "encoder_inputs_emb = encoder_embedding_layer(encoder_inputs)\n",
        "encoder = LSTM(LSTM_NEURONS, return_state=True)\n",
        "encoder_outputs, h, c = encoder(encoder_inputs_emb)\n",
        "encoder_states = [h, c]\n",
        "\n",
        "decoder_inputs = Input(shape=(outputs_maxlen,))\n",
        "decoder_inputs_emb = decoder_embedding_layer(decoder_inputs)\n",
        "decoder = LSTM(LSTM_NEURONS, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder(decoder_inputs_emb, initial_state=encoder_states)\n",
        "\n",
        "output_dense_layer = Dense(outputs_numwords, activation='softmax')\n",
        "outputs = output_dense_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM4mF-8udB5j",
        "outputId": "1fb27dc7-500a-4689-f24e-32bbc3262ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From s:\\Thesis_Work\\tff\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 3)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 3, 5)                 30        ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 5, 5)                 40        ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 2),                  64        ['embedding[0][0]']           \n",
            "                              (None, 2),                                                          \n",
            "                              (None, 2)]                                                          \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 5, 2),               64        ['embedding_1[0][0]',         \n",
            "                              (None, 2),                             'lstm[0][1]',                \n",
            "                              (None, 2)]                             'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 5, 8)                 24        ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 222 (888.00 Byte)\n",
            "Trainable params: 222 (888.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Model\n",
        "\n",
        "# defined [input] & output >>>>>>>>>>>>>>>\n",
        "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fCmHl8gAdB5k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "decoder_outputs_onehot  = tf.one_hot(decoder_output_sequences,\n",
        "\n",
        "depth=outputs_numwords #this param is important!!!\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuWPtj7YeOSm",
        "outputId": "c8c736d2-f6df-4a0e-d5f3-2180ca1b6ab8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(3, 5, 8), dtype=float32, numpy=\n",
              " array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(5, 8), dtype=float32, numpy=\n",
              " array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>,\n",
              " TensorShape([3, 5, 8]))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoder_outputs_onehot,decoder_outputs_onehot[0],decoder_outputs_onehot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9_8BCA0odB5k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From s:\\Thesis_Work\\tff\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From s:\\Thesis_Work\\tff\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.0806 - accuracy: 0.0667\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0754 - accuracy: 0.2667\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0716 - accuracy: 0.2667\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0684 - accuracy: 0.4000\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0655 - accuracy: 0.4667\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0628 - accuracy: 0.4667\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0602 - accuracy: 0.4667\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0578 - accuracy: 0.4667\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0555 - accuracy: 0.4667\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 2.0532 - accuracy: 0.4667\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0509 - accuracy: 0.4667\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0487 - accuracy: 0.5333\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.0465 - accuracy: 0.5333\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0443 - accuracy: 0.5333\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0421 - accuracy: 0.5333\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0400 - accuracy: 0.5333\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0379 - accuracy: 0.5333\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0357 - accuracy: 0.5333\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 2.0336 - accuracy: 0.5333\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 2.0314 - accuracy: 0.5333\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.0293 - accuracy: 0.5333\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 2.0271 - accuracy: 0.5333\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 2.0250 - accuracy: 0.5333\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0228 - accuracy: 0.5333\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0206 - accuracy: 0.5333\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0184 - accuracy: 0.5333\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0162 - accuracy: 0.5333\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0140 - accuracy: 0.5333\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0118 - accuracy: 0.5333\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0096 - accuracy: 0.5333\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0073 - accuracy: 0.5333\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.0051 - accuracy: 0.5333\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0028 - accuracy: 0.5333\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0005 - accuracy: 0.5333\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9982 - accuracy: 0.5333\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9959 - accuracy: 0.5333\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.9936 - accuracy: 0.5333\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9912 - accuracy: 0.5333\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9889 - accuracy: 0.5333\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9865 - accuracy: 0.5333\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9841 - accuracy: 0.5333\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.9817 - accuracy: 0.5333\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9793 - accuracy: 0.5333\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9768 - accuracy: 0.5333\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9744 - accuracy: 0.5333\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9719 - accuracy: 0.5333\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.9694 - accuracy: 0.5333\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.9669 - accuracy: 0.5333\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9644 - accuracy: 0.5333\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9619 - accuracy: 0.5333\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9593 - accuracy: 0.5333\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9568 - accuracy: 0.5333\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.9542 - accuracy: 0.5333\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9516 - accuracy: 0.5333\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.9490 - accuracy: 0.5333\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9464 - accuracy: 0.5333\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9437 - accuracy: 0.5333\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9411 - accuracy: 0.5333\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 400us/step - loss: 1.9384 - accuracy: 0.5333\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9357 - accuracy: 0.5333\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.9331 - accuracy: 0.5333\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9303 - accuracy: 0.5333\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.9276 - accuracy: 0.5333\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9249 - accuracy: 0.5333\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9221 - accuracy: 0.5333\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9194 - accuracy: 0.5333\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.9166 - accuracy: 0.5333\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9138 - accuracy: 0.5333\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9110 - accuracy: 0.5333\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.9082 - accuracy: 0.5333\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.9054 - accuracy: 0.5333\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9026 - accuracy: 0.5333\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.8997 - accuracy: 0.5333\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8969 - accuracy: 0.5333\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8940 - accuracy: 0.5333\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8911 - accuracy: 0.5333\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8882 - accuracy: 0.5333\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8854 - accuracy: 0.5333\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8825 - accuracy: 0.5333\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8795 - accuracy: 0.5333\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8766 - accuracy: 0.5333\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8737 - accuracy: 0.5333\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8708 - accuracy: 0.5333\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.8679 - accuracy: 0.5333\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.8649 - accuracy: 0.5333\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8620 - accuracy: 0.5333\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8590 - accuracy: 0.5333\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8561 - accuracy: 0.5333\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8531 - accuracy: 0.5333\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.8502 - accuracy: 0.5333\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8472 - accuracy: 0.5333\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.8443 - accuracy: 0.5333\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8413 - accuracy: 0.5333\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.8383 - accuracy: 0.5333\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8354 - accuracy: 0.5333\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8324 - accuracy: 0.5333\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8294 - accuracy: 0.5333\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8265 - accuracy: 0.5333\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8235 - accuracy: 0.5333\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8206 - accuracy: 0.5333\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8176 - accuracy: 0.5333\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8146 - accuracy: 0.6000\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.8117 - accuracy: 0.6000\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.8087 - accuracy: 0.6000\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.8058 - accuracy: 0.6000\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 873us/step - loss: 1.8028 - accuracy: 0.6000\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.7999 - accuracy: 0.6000\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.7969 - accuracy: 0.6000\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.7940 - accuracy: 0.6000\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7911 - accuracy: 0.6000\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7881 - accuracy: 0.6000\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.7852 - accuracy: 0.6000\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7823 - accuracy: 0.6000\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7794 - accuracy: 0.6000\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.7765 - accuracy: 0.6000\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.7736 - accuracy: 0.6000\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.7707 - accuracy: 0.6000\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.7678 - accuracy: 0.6000\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.7649 - accuracy: 0.6000\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7620 - accuracy: 0.6000\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.7592 - accuracy: 0.6000\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.7563 - accuracy: 0.6000\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7534 - accuracy: 0.6000\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7506 - accuracy: 0.6000\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.7478 - accuracy: 0.6000\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.7449 - accuracy: 0.6000\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.7421 - accuracy: 0.6000\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7393 - accuracy: 0.6000\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.7365 - accuracy: 0.6000\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7337 - accuracy: 0.6000\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7309 - accuracy: 0.6000\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7281 - accuracy: 0.6000\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.7253 - accuracy: 0.6000\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7225 - accuracy: 0.6000\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.7198 - accuracy: 0.6000\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7170 - accuracy: 0.6000\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.7143 - accuracy: 0.6000\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7115 - accuracy: 0.6000\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7088 - accuracy: 0.6000\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.7061 - accuracy: 0.6000\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.7034 - accuracy: 0.6000\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.7007 - accuracy: 0.6000\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6980 - accuracy: 0.6000\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6953 - accuracy: 0.6000\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6926 - accuracy: 0.6000\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.6899 - accuracy: 0.6000\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.6872 - accuracy: 0.6000\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.6846 - accuracy: 0.5333\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6819 - accuracy: 0.5333\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6793 - accuracy: 0.5333\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6767 - accuracy: 0.5333\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6741 - accuracy: 0.5333\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.6714 - accuracy: 0.5333\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 872us/step - loss: 1.6688 - accuracy: 0.5333\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 977us/step - loss: 1.6663 - accuracy: 0.5333\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6637 - accuracy: 0.5333\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6611 - accuracy: 0.5333\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6585 - accuracy: 0.5333\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6560 - accuracy: 0.5333\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6534 - accuracy: 0.5333\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6509 - accuracy: 0.5333\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6484 - accuracy: 0.5333\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6459 - accuracy: 0.5333\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.6434 - accuracy: 0.5333\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6409 - accuracy: 0.5333\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.6384 - accuracy: 0.5333\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6359 - accuracy: 0.5333\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.6335 - accuracy: 0.5333\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6310 - accuracy: 0.5333\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6286 - accuracy: 0.5333\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6261 - accuracy: 0.5333\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.6237 - accuracy: 0.5333\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6213 - accuracy: 0.5333\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.6189 - accuracy: 0.5333\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.6165 - accuracy: 0.5333\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6141 - accuracy: 0.5333\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.6117 - accuracy: 0.5333\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6094 - accuracy: 0.5333\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6070 - accuracy: 0.5333\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.6047 - accuracy: 0.5333\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6023 - accuracy: 0.5333\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.6000 - accuracy: 0.5333\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5977 - accuracy: 0.5333\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5954 - accuracy: 0.5333\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5931 - accuracy: 0.5333\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5908 - accuracy: 0.5333\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5885 - accuracy: 0.5333\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.5862 - accuracy: 0.5333\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5840 - accuracy: 0.5333\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5817 - accuracy: 0.5333\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5795 - accuracy: 0.5333\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5772 - accuracy: 0.5333\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.5750 - accuracy: 0.5333\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5728 - accuracy: 0.5333\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5706 - accuracy: 0.5333\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5684 - accuracy: 0.5333\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5662 - accuracy: 0.5333\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.5640 - accuracy: 0.5333\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5618 - accuracy: 0.5333\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.5596 - accuracy: 0.5333\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.5574 - accuracy: 0.5333\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5553 - accuracy: 0.5333\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5531 - accuracy: 0.5333\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.5510 - accuracy: 0.5333\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5488 - accuracy: 0.5333\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.5467 - accuracy: 0.5333\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5446 - accuracy: 0.5333\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.5424 - accuracy: 0.5333\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5403 - accuracy: 0.5333\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.5382 - accuracy: 0.5333\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.5361 - accuracy: 0.5333\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5340 - accuracy: 0.5333\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5319 - accuracy: 0.5333\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5299 - accuracy: 0.5333\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5278 - accuracy: 0.5333\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.5257 - accuracy: 0.5333\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5237 - accuracy: 0.5333\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5216 - accuracy: 0.5333\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5196 - accuracy: 0.5333\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5175 - accuracy: 0.5333\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.5155 - accuracy: 0.5333\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5135 - accuracy: 0.5333\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.5114 - accuracy: 0.5333\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.5094 - accuracy: 0.5333\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5074 - accuracy: 0.5333\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.5054 - accuracy: 0.5333\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5034 - accuracy: 0.5333\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.5014 - accuracy: 0.5333\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4994 - accuracy: 0.5333\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4975 - accuracy: 0.5333\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4955 - accuracy: 0.5333\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4935 - accuracy: 0.5333\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4916 - accuracy: 0.5333\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4896 - accuracy: 0.5333\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4877 - accuracy: 0.5333\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4857 - accuracy: 0.5333\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4838 - accuracy: 0.5333\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4819 - accuracy: 0.5333\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4799 - accuracy: 0.5333\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4780 - accuracy: 0.5333\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4761 - accuracy: 0.5333\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4742 - accuracy: 0.5333\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4723 - accuracy: 0.5333\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4704 - accuracy: 0.5333\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4685 - accuracy: 0.5333\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4666 - accuracy: 0.5333\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4647 - accuracy: 0.5333\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.4628 - accuracy: 0.5333\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.4610 - accuracy: 0.5333\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4591 - accuracy: 0.5333\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4572 - accuracy: 0.5333\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4554 - accuracy: 0.5333\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4535 - accuracy: 0.5333\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4517 - accuracy: 0.5333\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4498 - accuracy: 0.5333\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4480 - accuracy: 0.5333\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4461 - accuracy: 0.5333\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4443 - accuracy: 0.5333\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4425 - accuracy: 0.5333\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4407 - accuracy: 0.5333\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4388 - accuracy: 0.5333\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4370 - accuracy: 0.5333\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4352 - accuracy: 0.5333\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.4334 - accuracy: 0.5333\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4316 - accuracy: 0.5333\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4298 - accuracy: 0.5333\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4280 - accuracy: 0.6000\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4262 - accuracy: 0.6000\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4244 - accuracy: 0.6000\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4226 - accuracy: 0.6000\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4208 - accuracy: 0.6000\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.4191 - accuracy: 0.5333\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.4173 - accuracy: 0.5333\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.4155 - accuracy: 0.5333\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4138 - accuracy: 0.5333\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.4120 - accuracy: 0.5333\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.4102 - accuracy: 0.5333\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4085 - accuracy: 0.5333\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4067 - accuracy: 0.5333\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4050 - accuracy: 0.5333\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4033 - accuracy: 0.5333\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4015 - accuracy: 0.5333\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3998 - accuracy: 0.5333\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3980 - accuracy: 0.5333\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3963 - accuracy: 0.5333\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3946 - accuracy: 0.5333\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3929 - accuracy: 0.5333\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3912 - accuracy: 0.5333\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3894 - accuracy: 0.5333\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3877 - accuracy: 0.5333\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3860 - accuracy: 0.5333\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3843 - accuracy: 0.5333\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3826 - accuracy: 0.5333\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3809 - accuracy: 0.5333\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3792 - accuracy: 0.5333\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3776 - accuracy: 0.5333\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3759 - accuracy: 0.5333\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.3742 - accuracy: 0.5333\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3725 - accuracy: 0.5333\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3708 - accuracy: 0.5333\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3692 - accuracy: 0.5333\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3675 - accuracy: 0.5333\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3658 - accuracy: 0.5333\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3642 - accuracy: 0.5333\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3625 - accuracy: 0.5333\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.3609 - accuracy: 0.5333\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.3592 - accuracy: 0.5333\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3576 - accuracy: 0.5333\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3559 - accuracy: 0.5333\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3543 - accuracy: 0.5333\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3527 - accuracy: 0.5333\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3510 - accuracy: 0.5333\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3494 - accuracy: 0.5333\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3478 - accuracy: 0.5333\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3462 - accuracy: 0.5333\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.3445 - accuracy: 0.5333\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.3429 - accuracy: 0.5333\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3413 - accuracy: 0.5333\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3397 - accuracy: 0.5333\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3381 - accuracy: 0.5333\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3365 - accuracy: 0.5333\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 951us/step - loss: 1.3349 - accuracy: 0.5333\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3333 - accuracy: 0.5333\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3317 - accuracy: 0.5333\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3301 - accuracy: 0.5333\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3286 - accuracy: 0.5333\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3270 - accuracy: 0.5333\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.3254 - accuracy: 0.5333\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3238 - accuracy: 0.5333\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.3223 - accuracy: 0.5333\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3207 - accuracy: 0.5333\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3191 - accuracy: 0.5333\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 666us/step - loss: 1.3176 - accuracy: 0.5333\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.3160 - accuracy: 0.5333\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3145 - accuracy: 0.5333\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3129 - accuracy: 0.5333\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3114 - accuracy: 0.5333\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3098 - accuracy: 0.5333\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.3083 - accuracy: 0.5333\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3068 - accuracy: 0.5333\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3052 - accuracy: 0.5333\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3037 - accuracy: 0.5333\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3022 - accuracy: 0.5333\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.3007 - accuracy: 0.5333\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2991 - accuracy: 0.5333\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2976 - accuracy: 0.5333\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2961 - accuracy: 0.5333\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2946 - accuracy: 0.5333\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2931 - accuracy: 0.5333\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2916 - accuracy: 0.5333\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2901 - accuracy: 0.5333\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2886 - accuracy: 0.5333\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2871 - accuracy: 0.5333\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2856 - accuracy: 0.5333\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2841 - accuracy: 0.5333\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2827 - accuracy: 0.5333\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2812 - accuracy: 0.5333\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2797 - accuracy: 0.5333\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2782 - accuracy: 0.5333\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2768 - accuracy: 0.5333\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2753 - accuracy: 0.5333\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2738 - accuracy: 0.5333\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2724 - accuracy: 0.5333\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2709 - accuracy: 0.5333\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2695 - accuracy: 0.5333\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2680 - accuracy: 0.5333\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2666 - accuracy: 0.5333\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.2651 - accuracy: 0.5333\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2637 - accuracy: 0.5333\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2623 - accuracy: 0.5333\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.2608 - accuracy: 0.5333\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2594 - accuracy: 0.5333\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2580 - accuracy: 0.5333\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2565 - accuracy: 0.5333\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2551 - accuracy: 0.5333\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2537 - accuracy: 0.5333\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2523 - accuracy: 0.5333\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2508 - accuracy: 0.5333\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2494 - accuracy: 0.5333\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2480 - accuracy: 0.5333\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2466 - accuracy: 0.5333\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2452 - accuracy: 0.5333\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.2438 - accuracy: 0.5333\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2424 - accuracy: 0.5333\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2410 - accuracy: 0.5333\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2396 - accuracy: 0.6000\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2382 - accuracy: 0.6000\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2368 - accuracy: 0.6000\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.2354 - accuracy: 0.6000\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2340 - accuracy: 0.6000\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2326 - accuracy: 0.6000\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2313 - accuracy: 0.6000\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2299 - accuracy: 0.6000\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2285 - accuracy: 0.6000\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2271 - accuracy: 0.6000\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2257 - accuracy: 0.6000\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2244 - accuracy: 0.6000\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2230 - accuracy: 0.6000\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2216 - accuracy: 0.6000\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2203 - accuracy: 0.6000\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2189 - accuracy: 0.6000\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2175 - accuracy: 0.6000\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2162 - accuracy: 0.6000\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2148 - accuracy: 0.6000\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2135 - accuracy: 0.6000\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.2121 - accuracy: 0.6000\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2108 - accuracy: 0.6000\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2094 - accuracy: 0.6000\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2081 - accuracy: 0.6000\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2067 - accuracy: 0.6000\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.2054 - accuracy: 0.6000\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2040 - accuracy: 0.6000\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.2027 - accuracy: 0.6000\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2014 - accuracy: 0.6000\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.2000 - accuracy: 0.6000\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1987 - accuracy: 0.6000\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1973 - accuracy: 0.6000\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1960 - accuracy: 0.6000\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1947 - accuracy: 0.6000\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1934 - accuracy: 0.6000\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1920 - accuracy: 0.6000\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.1907 - accuracy: 0.6000\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1894 - accuracy: 0.6000\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1881 - accuracy: 0.6000\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1867 - accuracy: 0.6000\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1854 - accuracy: 0.6000\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1841 - accuracy: 0.6000\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1828 - accuracy: 0.6000\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1815 - accuracy: 0.6000\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1802 - accuracy: 0.6000\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1789 - accuracy: 0.6000\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1776 - accuracy: 0.6000\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1762 - accuracy: 0.6000\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1749 - accuracy: 0.6000\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1736 - accuracy: 0.6000\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.1723 - accuracy: 0.6000\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1710 - accuracy: 0.6000\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1697 - accuracy: 0.6000\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1684 - accuracy: 0.6000\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1672 - accuracy: 0.6000\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1659 - accuracy: 0.6000\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1646 - accuracy: 0.6000\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1633 - accuracy: 0.6000\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1620 - accuracy: 0.6000\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1607 - accuracy: 0.6000\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1594 - accuracy: 0.6000\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1581 - accuracy: 0.6000\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1568 - accuracy: 0.6000\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1556 - accuracy: 0.6000\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1543 - accuracy: 0.6000\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1530 - accuracy: 0.6000\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1517 - accuracy: 0.6000\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1505 - accuracy: 0.6000\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1492 - accuracy: 0.6000\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1479 - accuracy: 0.6000\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1466 - accuracy: 0.6000\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1454 - accuracy: 0.6000\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1441 - accuracy: 0.6000\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.1428 - accuracy: 0.6000\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1416 - accuracy: 0.6000\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1403 - accuracy: 0.6000\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.1390 - accuracy: 0.6000\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1378 - accuracy: 0.6000\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1365 - accuracy: 0.6000\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1352 - accuracy: 0.6000\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1340 - accuracy: 0.6000\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1327 - accuracy: 0.6000\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1315 - accuracy: 0.6000\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1302 - accuracy: 0.6000\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1290 - accuracy: 0.6000\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1277 - accuracy: 0.6000\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 901us/step - loss: 1.1265 - accuracy: 0.6000\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1252 - accuracy: 0.6000\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1240 - accuracy: 0.6000\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1227 - accuracy: 0.6000\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1215 - accuracy: 0.6000\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1202 - accuracy: 0.6000\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1190 - accuracy: 0.6000\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1177 - accuracy: 0.6000\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1165 - accuracy: 0.6000\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1153 - accuracy: 0.6000\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1140 - accuracy: 0.6000\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1128 - accuracy: 0.6000\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1115 - accuracy: 0.6000\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 1.1103 - accuracy: 0.6000\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1091 - accuracy: 0.6000\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1078 - accuracy: 0.6000\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1066 - accuracy: 0.6000\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1054 - accuracy: 0.6000\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1041 - accuracy: 0.6000\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1029 - accuracy: 0.6000\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.1017 - accuracy: 0.6000\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1004 - accuracy: 0.6000\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0992 - accuracy: 0.6000\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0980 - accuracy: 0.6000\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0968 - accuracy: 0.6000\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0955 - accuracy: 0.6000\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0943 - accuracy: 0.6000\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0931 - accuracy: 0.6000\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0919 - accuracy: 0.6000\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0907 - accuracy: 0.6000\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0894 - accuracy: 0.6000\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0882 - accuracy: 0.6000\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0870 - accuracy: 0.6000\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0858 - accuracy: 0.6000\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0846 - accuracy: 0.6000\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0834 - accuracy: 0.6667\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0822 - accuracy: 0.6667\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0810 - accuracy: 0.6667\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0798 - accuracy: 0.6667\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0785 - accuracy: 0.6667\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0773 - accuracy: 0.6667\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0761 - accuracy: 0.6667\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0749 - accuracy: 0.6667\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0737 - accuracy: 0.6667\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0725 - accuracy: 0.6667\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0713 - accuracy: 0.6667\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0701 - accuracy: 0.6667\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0689 - accuracy: 0.6667\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0677 - accuracy: 0.6667\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0665 - accuracy: 0.6667\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0653 - accuracy: 0.6667\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0641 - accuracy: 0.6667\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0629 - accuracy: 0.6667\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0618 - accuracy: 0.7333\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0606 - accuracy: 0.7333\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0594 - accuracy: 0.7333\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0582 - accuracy: 0.7333\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0570 - accuracy: 0.7333\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0558 - accuracy: 0.7333\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0546 - accuracy: 0.8000\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0534 - accuracy: 0.8000\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0523 - accuracy: 0.8000\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0511 - accuracy: 0.8000\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0499 - accuracy: 0.8000\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 1.0487 - accuracy: 0.8000\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0475 - accuracy: 0.8000\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0464 - accuracy: 0.8000\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0452 - accuracy: 0.8000\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0440 - accuracy: 0.8000\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0428 - accuracy: 0.8000\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0416 - accuracy: 0.8000\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0405 - accuracy: 0.8000\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0393 - accuracy: 0.8000\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0381 - accuracy: 0.8000\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0370 - accuracy: 0.8000\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0358 - accuracy: 0.8000\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0346 - accuracy: 0.8000\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0335 - accuracy: 0.8000\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0323 - accuracy: 0.8000\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0311 - accuracy: 0.8000\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0300 - accuracy: 0.8000\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0288 - accuracy: 0.8000\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.0276 - accuracy: 0.8000\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0265 - accuracy: 0.8000\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0253 - accuracy: 0.8000\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0242 - accuracy: 0.8000\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0230 - accuracy: 0.8000\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0218 - accuracy: 0.8000\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0207 - accuracy: 0.8000\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0195 - accuracy: 0.8000\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0184 - accuracy: 0.8000\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0172 - accuracy: 0.8000\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0161 - accuracy: 0.8000\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0149 - accuracy: 0.8000\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0138 - accuracy: 0.8000\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.0126 - accuracy: 0.8000\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0115 - accuracy: 0.8000\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0103 - accuracy: 0.8000\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0092 - accuracy: 0.8000\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0081 - accuracy: 0.8000\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0069 - accuracy: 0.8000\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0058 - accuracy: 0.8000\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0046 - accuracy: 0.8000\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 1.0035 - accuracy: 0.8000\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0024 - accuracy: 0.8000\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0012 - accuracy: 0.8000\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0001 - accuracy: 0.8000\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9990 - accuracy: 0.8000\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9978 - accuracy: 0.8000\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9967 - accuracy: 0.8000\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9956 - accuracy: 0.8000\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9944 - accuracy: 0.8000\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9933 - accuracy: 0.8000\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.9922 - accuracy: 0.8000\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9911 - accuracy: 0.8000\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9899 - accuracy: 0.8000\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9888 - accuracy: 0.8000\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9877 - accuracy: 0.8000\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9866 - accuracy: 0.8000\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9855 - accuracy: 0.8000\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9843 - accuracy: 0.8000\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9832 - accuracy: 0.8000\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9821 - accuracy: 0.8000\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9810 - accuracy: 0.8000\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9799 - accuracy: 0.8000\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9788 - accuracy: 0.8000\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9777 - accuracy: 0.8000\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9765 - accuracy: 0.8000\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9754 - accuracy: 0.8000\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9743 - accuracy: 0.8000\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9732 - accuracy: 0.8000\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9721 - accuracy: 0.8000\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9710 - accuracy: 0.8000\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9699 - accuracy: 0.8000\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.9688 - accuracy: 0.8000\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9677 - accuracy: 0.8000\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9666 - accuracy: 0.8000\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9655 - accuracy: 0.8000\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9644 - accuracy: 0.8000\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9633 - accuracy: 0.8000\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9622 - accuracy: 0.8000\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.9611 - accuracy: 0.8000\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9600 - accuracy: 0.8000\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9589 - accuracy: 0.8000\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9578 - accuracy: 0.8000\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9568 - accuracy: 0.8000\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9557 - accuracy: 0.8000\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9546 - accuracy: 0.8000\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9535 - accuracy: 0.8000\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9524 - accuracy: 0.8000\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9513 - accuracy: 0.8000\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9502 - accuracy: 0.8000\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9492 - accuracy: 0.8000\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9481 - accuracy: 0.8000\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9470 - accuracy: 0.8000\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9459 - accuracy: 0.8000\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9449 - accuracy: 0.8000\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9438 - accuracy: 0.8000\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.9427 - accuracy: 0.8000\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9416 - accuracy: 0.8000\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9406 - accuracy: 0.8000\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9395 - accuracy: 0.8000\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9384 - accuracy: 0.8000\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9374 - accuracy: 0.8000\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9363 - accuracy: 0.8000\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9352 - accuracy: 0.8000\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9342 - accuracy: 0.8000\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9331 - accuracy: 0.8000\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9320 - accuracy: 0.8000\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.9310 - accuracy: 0.8000\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9299 - accuracy: 0.8000\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9288 - accuracy: 0.8000\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9278 - accuracy: 0.8000\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9267 - accuracy: 0.8000\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9257 - accuracy: 0.8000\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9246 - accuracy: 0.8000\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9236 - accuracy: 0.8000\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.9225 - accuracy: 0.8000\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9215 - accuracy: 0.8000\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9204 - accuracy: 0.8000\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.9194 - accuracy: 0.8000\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9183 - accuracy: 0.8000\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.9173 - accuracy: 0.8667\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9162 - accuracy: 0.8667\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9152 - accuracy: 0.8667\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9141 - accuracy: 0.8667\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9131 - accuracy: 0.8667\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9120 - accuracy: 0.8667\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9110 - accuracy: 0.8667\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9100 - accuracy: 0.8667\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9089 - accuracy: 0.8667\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9079 - accuracy: 0.8667\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9068 - accuracy: 0.8667\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9058 - accuracy: 0.8667\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9048 - accuracy: 0.8667\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9037 - accuracy: 0.8667\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9027 - accuracy: 0.8667\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.9017 - accuracy: 0.8667\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9006 - accuracy: 0.8667\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8996 - accuracy: 0.8667\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8986 - accuracy: 0.8667\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8975 - accuracy: 0.8667\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.8965 - accuracy: 0.8667\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.8955 - accuracy: 0.8667\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8944 - accuracy: 0.8667\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8934 - accuracy: 0.8667\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8924 - accuracy: 0.8667\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8914 - accuracy: 0.8667\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8904 - accuracy: 0.8667\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.8893 - accuracy: 0.8667\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8883 - accuracy: 0.8667\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8873 - accuracy: 0.8667\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8863 - accuracy: 0.8667\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8853 - accuracy: 0.8667\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8842 - accuracy: 0.8667\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8832 - accuracy: 0.8667\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8822 - accuracy: 0.8667\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8812 - accuracy: 0.8667\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8802 - accuracy: 0.8667\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8792 - accuracy: 0.8667\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8782 - accuracy: 0.8667\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8772 - accuracy: 0.8667\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8761 - accuracy: 0.8667\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8751 - accuracy: 0.8667\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8741 - accuracy: 0.8667\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8731 - accuracy: 0.8667\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8721 - accuracy: 0.8667\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8711 - accuracy: 0.8667\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8701 - accuracy: 0.8667\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8691 - accuracy: 0.8667\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8681 - accuracy: 0.8667\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8671 - accuracy: 0.8667\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8661 - accuracy: 0.8667\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8651 - accuracy: 0.8667\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8641 - accuracy: 0.8667\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8631 - accuracy: 0.8667\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8621 - accuracy: 0.8667\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8611 - accuracy: 0.8667\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8602 - accuracy: 0.8667\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8592 - accuracy: 0.8667\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8582 - accuracy: 0.8667\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8572 - accuracy: 0.8667\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8562 - accuracy: 0.8667\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8552 - accuracy: 0.8667\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8542 - accuracy: 0.8667\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8532 - accuracy: 0.8667\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.8523 - accuracy: 0.8667\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8513 - accuracy: 0.8667\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8503 - accuracy: 0.8667\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8493 - accuracy: 0.8667\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8483 - accuracy: 0.8667\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8474 - accuracy: 0.8667\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8464 - accuracy: 0.8667\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.8454 - accuracy: 0.8667\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8444 - accuracy: 0.8667\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8435 - accuracy: 0.8667\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8425 - accuracy: 0.8667\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8415 - accuracy: 0.8667\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.8405 - accuracy: 0.8667\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8396 - accuracy: 0.8667\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8386 - accuracy: 0.8667\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8376 - accuracy: 0.8667\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8367 - accuracy: 0.8667\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8357 - accuracy: 0.8667\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8347 - accuracy: 0.8667\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8338 - accuracy: 0.8667\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8328 - accuracy: 0.8667\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8318 - accuracy: 0.8667\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8309 - accuracy: 0.8667\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8299 - accuracy: 0.8667\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8289 - accuracy: 0.8667\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8280 - accuracy: 0.8667\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.8270 - accuracy: 0.8667\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8261 - accuracy: 0.8667\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8251 - accuracy: 0.8667\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.8241 - accuracy: 0.8667\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8232 - accuracy: 0.8667\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8222 - accuracy: 0.8667\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8213 - accuracy: 0.8667\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8203 - accuracy: 0.8667\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8194 - accuracy: 0.8667\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.8184 - accuracy: 0.8667\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8175 - accuracy: 0.8667\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.8165 - accuracy: 0.8667\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8156 - accuracy: 0.8667\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8146 - accuracy: 0.8667\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8137 - accuracy: 0.8667\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.8127 - accuracy: 0.8667\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8118 - accuracy: 0.8667\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.8109 - accuracy: 0.8667\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8099 - accuracy: 0.8667\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 753us/step - loss: 0.8090 - accuracy: 0.8667\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.8080 - accuracy: 0.8667\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8071 - accuracy: 0.8667\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8062 - accuracy: 0.8667\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.8052 - accuracy: 0.8667\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.8043 - accuracy: 0.8667\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8034 - accuracy: 0.8667\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8024 - accuracy: 0.8667\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8015 - accuracy: 0.8667\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8006 - accuracy: 0.8667\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7996 - accuracy: 0.8667\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7987 - accuracy: 0.8667\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7978 - accuracy: 0.8667\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 994us/step - loss: 0.7969 - accuracy: 0.8667\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7959 - accuracy: 0.8667\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7950 - accuracy: 0.8667\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7941 - accuracy: 0.8667\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7932 - accuracy: 0.8667\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7922 - accuracy: 0.8667\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7913 - accuracy: 0.8667\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7904 - accuracy: 0.8667\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7895 - accuracy: 0.8667\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7886 - accuracy: 0.8667\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7876 - accuracy: 0.8667\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7867 - accuracy: 0.8667\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7858 - accuracy: 0.8667\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7849 - accuracy: 0.8667\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7840 - accuracy: 0.8667\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7831 - accuracy: 0.8667\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7822 - accuracy: 0.8667\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7813 - accuracy: 0.8667\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7804 - accuracy: 0.8667\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7795 - accuracy: 0.8667\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7786 - accuracy: 0.8667\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7777 - accuracy: 0.8667\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7768 - accuracy: 0.8667\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7759 - accuracy: 0.8667\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7750 - accuracy: 0.8667\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7741 - accuracy: 0.8667\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7732 - accuracy: 0.8667\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7723 - accuracy: 0.8667\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7714 - accuracy: 0.8667\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7705 - accuracy: 0.8667\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7696 - accuracy: 0.8667\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7687 - accuracy: 0.8667\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7678 - accuracy: 0.8667\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7669 - accuracy: 0.8667\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7660 - accuracy: 0.8667\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7651 - accuracy: 0.8667\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7643 - accuracy: 0.8667\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7634 - accuracy: 0.8667\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7625 - accuracy: 0.8667\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7616 - accuracy: 0.8667\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7607 - accuracy: 0.8667\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7598 - accuracy: 0.8667\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7590 - accuracy: 0.8667\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7581 - accuracy: 0.8667\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7572 - accuracy: 0.8667\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.7563 - accuracy: 0.8667\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7555 - accuracy: 0.8667\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7546 - accuracy: 0.8667\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7537 - accuracy: 0.8667\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7529 - accuracy: 0.8667\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7520 - accuracy: 0.8667\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7511 - accuracy: 0.8667\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7503 - accuracy: 0.8667\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.7494 - accuracy: 0.8667\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7485 - accuracy: 0.8667\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7477 - accuracy: 0.8667\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7468 - accuracy: 0.8667\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7459 - accuracy: 0.8667\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7451 - accuracy: 0.8667\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7442 - accuracy: 0.8667\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7434 - accuracy: 0.8667\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7425 - accuracy: 0.8667\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7417 - accuracy: 0.8667\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7408 - accuracy: 0.8667\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7400 - accuracy: 0.8667\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7391 - accuracy: 0.8667\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7383 - accuracy: 0.8667\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7374 - accuracy: 0.8667\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7366 - accuracy: 0.8667\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7357 - accuracy: 0.8667\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7349 - accuracy: 0.8667\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7340 - accuracy: 0.8667\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7332 - accuracy: 0.8667\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.7324 - accuracy: 0.8667\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7315 - accuracy: 0.8667\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7307 - accuracy: 0.8667\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7299 - accuracy: 0.8667\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7290 - accuracy: 0.8667\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7282 - accuracy: 0.8667\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7274 - accuracy: 0.8667\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7265 - accuracy: 0.8667\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7257 - accuracy: 0.8667\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7249 - accuracy: 0.8667\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7240 - accuracy: 0.8667\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7232 - accuracy: 0.8667\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7224 - accuracy: 0.8667\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7216 - accuracy: 0.8667\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7207 - accuracy: 0.8667\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7199 - accuracy: 0.8667\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7191 - accuracy: 0.8667\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7183 - accuracy: 0.8667\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.7175 - accuracy: 0.8667\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7167 - accuracy: 0.8667\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7158 - accuracy: 0.8667\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7150 - accuracy: 0.8667\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7142 - accuracy: 0.8667\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7134 - accuracy: 0.8667\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7126 - accuracy: 0.8667\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7118 - accuracy: 0.8667\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7110 - accuracy: 0.8667\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7102 - accuracy: 0.8667\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7094 - accuracy: 0.8667\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7086 - accuracy: 0.8667\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7078 - accuracy: 0.8667\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 998us/step - loss: 0.7070 - accuracy: 0.8667\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7062 - accuracy: 0.8667\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7054 - accuracy: 0.8667\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7046 - accuracy: 0.8667\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7038 - accuracy: 0.8667\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7030 - accuracy: 0.8667\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7022 - accuracy: 0.8667\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.7014 - accuracy: 0.8667\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7007 - accuracy: 0.8667\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6999 - accuracy: 0.8667\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6991 - accuracy: 0.8667\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6983 - accuracy: 0.8667\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6975 - accuracy: 0.8667\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 957us/step - loss: 0.6967 - accuracy: 0.8667\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6960 - accuracy: 0.8667\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6952 - accuracy: 0.8667\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6944 - accuracy: 0.8667\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6936 - accuracy: 0.8667\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.8667\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6921 - accuracy: 0.8667\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6913 - accuracy: 0.8667\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6906 - accuracy: 0.8667\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6898 - accuracy: 0.8667\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6890 - accuracy: 0.8667\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6883 - accuracy: 0.8667\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6875 - accuracy: 0.8667\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6867 - accuracy: 0.8667\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6860 - accuracy: 0.8667\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6852 - accuracy: 0.8667\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6845 - accuracy: 0.8667\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6837 - accuracy: 0.8667\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6830 - accuracy: 0.8667\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6822 - accuracy: 0.8667\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6815 - accuracy: 0.8667\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6807 - accuracy: 0.8667\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6800 - accuracy: 0.8667\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6792 - accuracy: 0.8667\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6785 - accuracy: 0.8667\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6777 - accuracy: 0.8667\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6770 - accuracy: 0.8667\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6763 - accuracy: 0.8667\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6755 - accuracy: 0.8667\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6748 - accuracy: 0.8667\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6741 - accuracy: 0.8667\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6733 - accuracy: 0.8667\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6726 - accuracy: 0.8667\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6719 - accuracy: 0.8667\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6711 - accuracy: 0.8667\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6704 - accuracy: 0.8667\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6697 - accuracy: 0.8667\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6690 - accuracy: 0.8667\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6683 - accuracy: 0.8667\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6675 - accuracy: 0.8667\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6668 - accuracy: 0.8667\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6661 - accuracy: 0.8667\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6654 - accuracy: 0.8667\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6647 - accuracy: 0.8667\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6640 - accuracy: 0.8667\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6633 - accuracy: 0.8667\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6626 - accuracy: 0.8667\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6619 - accuracy: 0.8667\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6612 - accuracy: 0.8667\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6605 - accuracy: 0.8667\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6598 - accuracy: 0.8667\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6591 - accuracy: 0.8667\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6584 - accuracy: 0.8667\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6577 - accuracy: 0.8667\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6570 - accuracy: 0.8667\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6563 - accuracy: 0.8667\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6556 - accuracy: 0.8667\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6549 - accuracy: 0.8667\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6543 - accuracy: 0.8667\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6536 - accuracy: 0.8667\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.6529 - accuracy: 0.8667\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.6522 - accuracy: 0.8667\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6515 - accuracy: 0.8667\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6509 - accuracy: 0.8667\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6502 - accuracy: 0.8667\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6495 - accuracy: 0.8667\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6489 - accuracy: 0.8667\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6482 - accuracy: 0.8667\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6475 - accuracy: 0.8667\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6469 - accuracy: 0.8667\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6462 - accuracy: 0.8667\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6456 - accuracy: 0.8667\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 282us/step - loss: 0.6449 - accuracy: 0.8667\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6443 - accuracy: 0.8667\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6436 - accuracy: 0.8667\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6430 - accuracy: 0.8667\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6423 - accuracy: 0.8667\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6417 - accuracy: 0.8667\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6410 - accuracy: 0.8667\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6404 - accuracy: 0.8667\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6397 - accuracy: 0.8667\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6391 - accuracy: 0.8667\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6385 - accuracy: 0.8667\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6378 - accuracy: 0.8667\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6372 - accuracy: 0.8667\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6366 - accuracy: 0.8667\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6359 - accuracy: 0.8667\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6353 - accuracy: 0.8667\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6347 - accuracy: 0.8667\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6341 - accuracy: 0.8667\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.6334 - accuracy: 0.8667\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6328 - accuracy: 0.8667\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6322 - accuracy: 0.8667\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6316 - accuracy: 0.8667\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6310 - accuracy: 0.8667\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6304 - accuracy: 0.8667\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6298 - accuracy: 0.8667\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6292 - accuracy: 0.8667\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6285 - accuracy: 0.8667\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6279 - accuracy: 0.8667\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6273 - accuracy: 0.8667\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6267 - accuracy: 0.8667\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6261 - accuracy: 0.8667\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6255 - accuracy: 0.8667\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 0s/step - loss: 0.6250 - accuracy: 0.8667\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6244 - accuracy: 0.8667\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6238 - accuracy: 0.8667\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6232 - accuracy: 0.8667\n"
          ]
        }
      ],
      "source": [
        "trn = model.fit([encoder_input_sequences, decoder_input_sequences],\n",
        "\n",
        "               decoder_outputs_onehot, epochs=1000\n",
        "               )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bUn_JPSkdB5k"
      },
      "outputs": [],
      "source": [
        "# Iidx2wrd,Oidx2wrd,\n",
        "# output_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvV4FA05dB5k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8quU0HsdB5l"
      },
      "source": [
        "## THE_INFERENCE `is here boiis`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MchajIldB5l",
        "outputId": "1644535f-ed67-4729-b3cd-23e843c458cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[2, 4, 1]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_seq='how you hello'\n",
        "input_seq=input_tokenizer.texts_to_sequences([input_seq])\n",
        "input_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4eaXxgfdB5l",
        "outputId": "03803c96-e5c8-415d-93f6-ff506116d85b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 3)]               0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 3, 5)              30        \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 2),               64        \n",
            "                              (None, 2),                         \n",
            "                              (None, 2)]                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 94 (376.00 Byte)\n",
            "Trainable params: 94 (376.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "print(encoder_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPaxcbDhdB5l",
        "outputId": "1edab05a-78a1-4df2-af7e-2379c45b85f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     multiple                     40        ['input_5[0][0]']             \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)        [(None, 2)]                  0         []                            \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               multiple                     64        ['embedding_1[1][0]',         \n",
            "                                                                     'input_3[0][0]',             \n",
            "                                                                     'input_4[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)               multiple                     24        ['lstm_1[1][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 128 (512.00 Byte)\n",
            "Trainable params: 128 (512.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "decoder_input_h = Input(shape=(LSTM_NEURONS,))\n",
        "decoder_input_c = Input(shape=(LSTM_NEURONS,))\n",
        "decoder_input_states = [decoder_input_h, decoder_input_c]\n",
        "\n",
        "decoder_input_word = Input(shape=(1,))\n",
        "decoder_input_word_emb = decoder_embedding_layer(decoder_input_word)\n",
        "\n",
        "decoder_outputs, h, c = decoder(decoder_input_word_emb, initial_state=decoder_input_states)\n",
        "decoder_states = [h, c]\n",
        "\n",
        "outputs = output_dense_layer(decoder_outputs)\n",
        "decoder_model = Model([decoder_input_word]+decoder_input_states, [outputs]+decoder_states)\n",
        "print(decoder_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTw5-qBIdB5m",
        "outputId": "e7e48e91-6aa4-49a7-f7ff-2638136a3964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 616ms/step\n"
          ]
        }
      ],
      "source": [
        "states = encoder_model.predict(input_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni42I3W2dB5m",
        "outputId": "a2d42ec3-f5a9-4e92-808f-5ab0baafccfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[ 0.22516811, -0.6741042 ]], dtype=float32),\n",
              " array([[ 1.3700013, -1.3576834]], dtype=float32)]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSl79beBdB5m",
        "outputId": "a23f4430-d63a-4885-9aa4-50f7d63376df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sos= outputs_word2index['<sos>']\n",
        "eos = outputs_word2index['<eos>']\n",
        "sos,eos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnUElBmKdB5n",
        "outputId": "f6ea1b35-0f79-4d70-bc4d-dc0e5c9e2614"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import zeros,argmax,argsort\n",
        "\n",
        "output_seq = zeros((1, 1))\n",
        "output_seq[0, 0] = sos\n",
        "output_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Bt90nDpodB5n"
      },
      "outputs": [],
      "source": [
        "# after running all other\n",
        "# states = [h, c]\n",
        "# output_seq[0, 0] = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgaQdUFHdB5n",
        "outputId": "19d48180-506f-4f7b-9a58-6fa447332b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 793ms/step\n"
          ]
        }
      ],
      "source": [
        "output_tokens, h, c = decoder_model.predict([output_seq]+states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlEVXpdkdB5o",
        "outputId": "c644cf7b-2c13-41df-e456-0f94c1468030"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1, 1, 8),\n",
              " array([0.1077996 , 0.04039694, 0.0137523 , 0.01786779, 0.48442328,\n",
              "        0.26857722, 0.0555376 , 0.01164523], dtype=float32),\n",
              " array([[[0.1077996 , 0.04039694, 0.0137523 , 0.01786779, 0.48442328,\n",
              "          0.26857722, 0.0555376 , 0.01164523]]], dtype=float32))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_tokens.shape ,output_tokens[0,0,:] , output_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eS8QhDkdB5o",
        "outputId": "19dae5c7-a2cf-4b4c-be4a-f0b34db1d3d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.8588016  -0.72210854]]\n",
            "\n",
            "\n",
            "\n",
            "[[ 1.3478651  -0.95389736]]\n",
            "{1: '<sos>', 2: '<eos>', 3: 'ohe', 4: 'kemon', 5: 'acho', 6: 'tumi', 7: 'biday'}\n"
          ]
        }
      ],
      "source": [
        "print(h)\n",
        "print('\\n\\n')\n",
        "print(c)\n",
        "print(Oidx2wrd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zA6HRMLdB5o"
      },
      "source": [
        "### how are you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW4A9h4p3ksd",
        "outputId": "f696e5dd-8007-4609-de4c-1d1783958dba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_tokens\n",
        "idx = argmax(output_tokens)\n",
        "idx\n",
        "# word = Oidx2wrd[idx]\n",
        "# word,idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nl504ofdB5o",
        "outputId": "716f213c-9c56-4d4e-81ab-e3ee5671840b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 51ms/step\n",
            "[[[0.09054932 0.03386606 0.00755689 0.01148068 0.56718934 0.2452295\n",
            "   0.03712631 0.00700186]]]\n",
            "kemon 4 [[[7 2 3 1 6 0 5 4]]]\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "[[[0.03446657 0.04119627 0.03386435 0.02696405 0.15020889 0.3896563\n",
            "   0.30039018 0.02325344]]]\n",
            "acho 5 [[[7 3 2 0 1 4 6 5]]]\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "[[[0.01020189 0.02485483 0.04212608 0.0232145  0.0377858  0.28122532\n",
            "   0.55612713 0.02446449]]]\n",
            "tumi 6 [[[0 3 7 1 4 2 5 6]]]\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[[0.14459576 0.01365746 0.47367007 0.1353451  0.00768285 0.01874963\n",
            "   0.0525902  0.1537089 ]]]\n",
            "<eos> 2 [[[4 1 5 6 3 0 7 2]]]\n"
          ]
        }
      ],
      "source": [
        "word='<sos>'\n",
        "input_seq='how are you'\n",
        "input_seq=input_tokenizer.texts_to_sequences([input_seq])\n",
        "states = encoder_model.predict(input_seq)\n",
        "while word!='<eos>':\n",
        "\n",
        "  output_tokens, h, c = decoder_model.predict([output_seq]+states)\n",
        "  print(output_tokens)\n",
        "  idx,srt = argmax(output_tokens,),np.argsort(output_tokens)\n",
        "\n",
        "  output_seq[0, 0] = idx\n",
        "  states=[h,c]\n",
        "  word = Oidx2wrd[idx]\n",
        "\n",
        "  print(word,idx,srt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ax1__6ddB5p",
        "outputId": "08c991c6-078d-4873-c683-d8c9142c27d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[array([[1.]]), 0.0]]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoder_input = np.zeros((1, 1))\n",
        "decoder_input[0, 0] = sos\n",
        "\n",
        "sequences = [[decoder_input, 0.0]]\n",
        "sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdRl5np7dB5p"
      },
      "outputs": [],
      "source": [
        "\n",
        "def BeamSearch(states,decoder_model,outputs_maxlen,beam_size=2):\n",
        "  max_len=outputs_maxlen+1\n",
        "  sos=Oidx2wrd[1]\n",
        "  decoder_input = np.zeros((1, 1))\n",
        "  decoder_input[0, 0] = 1\n",
        "\n",
        "  sequences = [[decoder_input, 0.0]]\n",
        "\n",
        "  ln=sequences[0][0][0]\n",
        "\n",
        "  neWeight={0:states}\n",
        "  ii=0\n",
        "  while len(ln) < max_len:\n",
        "      all_candidates = []\n",
        "\n",
        "\n",
        "\n",
        "      for i in range(len(sequences)):\n",
        "          seq, score = sequences[i]\n",
        "\n",
        "\n",
        "          decoder_input[0,0]=seq[0][-1]\n",
        "          if ii!=0:\n",
        "            current_states = states[i]\n",
        "          else:current_states = states\n",
        "          # print('input >>>',seq, decoder_input,)\n",
        "\n",
        "          probs, h, c = decoder_model.predict([decoder_input]+current_states)\n",
        "          len(probs)\n",
        "          word_preds = np.argsort(probs)[0][0][-beam_size:]\n",
        "\n",
        "\n",
        "          for w in word_preds:\n",
        "              candidate_seq = [\n",
        "                  np.concatenate([seq, np.array([[w]])], axis=1),\n",
        "\n",
        "                              score\n",
        "                              - np.log(probs[-1][-1][w])]\n",
        "              new_states = [h, c]\n",
        "              all_candidates.append((candidate_seq, new_states))\n",
        "      ii+=1\n",
        "      all_candidates.sort(key=lambda tup: tup[0][1])\n",
        "      sequences = [tup[0] for tup in all_candidates]\n",
        "      states = [tup[1] for tup in all_candidates]\n",
        "\n",
        "\n",
        "\n",
        "      ln=(sequences[0][0][0])\n",
        "  min_array = min(sequences, key=lambda x: x[1])\n",
        "  return min_array[0],sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLI1eLYJjwOI"
      },
      "source": [
        "{1: '<sos>', 2: '<eos>', 3: 'ohe', 4: 'kemon', 5: 'acho', 6: 'tumi', 7: 'biday'}\n",
        "({1: 'hello', 2: 'how', 3: 'are', 4: 'you', 5: 'goodbye'},"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhcXLHGHPm00",
        "outputId": "e07da979-4b02-4f30-8222-6176b9a89135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 125ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        }
      ],
      "source": [
        "input_seq='how are you'\n",
        "input_seq=input_tokenizer.texts_to_sequences([input_seq])\n",
        "input_seq=[[2,3,4]]\n",
        "states = encoder_model.predict(input_seq)\n",
        "x,seq=BeamSearch(states,decoder_model ,outputs_maxlen,beam_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKEWolX3dB5q",
        "outputId": "fbb47563-5e31-45d5-89ad-4fbf4c754c15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[array([[1., 4., 5., 6., 2., 2.]]), 4.441096544265747],\n",
              " [array([[1., 4., 6., 3., 0., 0.]]), 4.769274711608887],\n",
              " [array([[1., 4., 5., 5., 6., 2.]]), 4.927106618881226],\n",
              " [array([[1., 4., 5., 6., 3., 2.]]), 5.052044034004211],\n",
              " [array([[1., 4., 5., 6., 2., 3.]]), 5.230000138282776],\n",
              " [array([[1., 4., 6., 2., 2., 2.]]), 5.4529993534088135],\n",
              " [array([[1., 5., 4., 5., 6., 2.]]), 5.554852485656738],\n",
              " [array([[1., 4., 6., 3., 2., 0.]]), 5.582513928413391],\n",
              " [array([[1., 4., 6., 2., 3., 0.]]), 5.734493017196655],\n",
              " [array([[1., 5., 4., 6., 2., 2.]]), 5.746296763420105],\n",
              " [array([[1., 4., 5., 5., 2., 2.]]), 5.808568358421326],\n",
              " [array([[1., 4., 5., 5., 2., 6.]]), 5.848628520965576],\n",
              " [array([[1., 4., 5., 5., 6., 3.]]), 5.94578230381012],\n",
              " [array([[1., 4., 5., 6., 3., 0.]]), 6.069006562232971],\n",
              " [array([[1., 4., 6., 2., 3., 2.]]), 6.072152376174927],\n",
              " [array([[1., 4., 6., 2., 2., 3.]]), 6.125104665756226],\n",
              " [array([[1., 5., 5., 6., 2., 2.]]), 6.136052429676056],\n",
              " [array([[1., 5., 5., 6., 0., 0.]]), 6.190252631902695],\n",
              " [array([[1., 5., 4., 5., 5., 2.]]), 6.233413875102997],\n",
              " [array([[1., 5., 5., 6., 2., 0.]]), 6.402964770793915],\n",
              " [array([[1., 5., 5., 5., 2., 2.]]), 6.440853595733643],\n",
              " [array([[1., 4., 6., 3., 2., 2.]]), 6.443949341773987],\n",
              " [array([[1., 4., 6., 3., 0., 2.]]), 6.4735302329063416],\n",
              " [array([[1., 5., 4., 6., 3., 2.]]), 6.483932316303253],\n",
              " [array([[1., 5., 4., 6., 2., 3.]]), 6.511916279792786],\n",
              " [array([[1., 5., 4., 5., 6., 3.]]), 6.532501816749573],\n",
              " [array([[1., 5., 4., 6., 3., 0.]]), 6.592488169670105],\n",
              " [array([[1., 5., 5., 5., 3., 2.]]), 6.874683320522308],\n",
              " [array([[1., 5., 4., 5., 5., 3.]]), 6.972115397453308],\n",
              " [array([[1., 5., 5., 5., 2., 3.]]), 7.156654477119446],\n",
              " [array([[1., 5., 5., 6., 0., 2.]]), 7.34115469455719],\n",
              " [array([[1., 5., 5., 5., 3., 0.]]), 7.82352602481842]]"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-22yLGTDKMlY",
        "outputId": "2af5db92-9f92-4e9c-d15a-598e89d6b404"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 4., 5., 6., 2., 2.]])"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# min_array = min(sequences, key=lambda x: x[1])\n",
        "# x=min_array[0]\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKFLbDK8KRM-",
        "outputId": "a859d6ad-9edd-4d5e-eae0-75bd457f5b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0 <sos>\n",
            "4.0 kemon\n",
            "5.0 acho\n",
            "6.0 tumi\n",
            "2.0 <eos>\n",
            "2.0 <eos>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for i in x[0]:\n",
        "    print(i,Oidx2wrd[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAzmkZwoOuvD"
      },
      "outputs": [],
      "source": [
        "start_seq = np.array([[1]])\n",
        "initial_states = [states] + [np.zeros((1, LSTM_NEURONS))] * 2  #\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kclojHSbyt-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7QYm8ercZZI"
      },
      "source": [
        "# TRANSFORMERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYJcsh5xcemY",
        "outputId": "abc7f4d6-09ab-4997-876b-d8d642619b07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 3)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_pairs=[]\n",
        "# [('He turned to look Langdon in the eye.',\n",
        "#   '[start] সে ঘুরে ল্যাংডনের চোখের দিকে তাকায়। [end]'),\n",
        "\n",
        "en_lst = ['hello', 'how are you', 'goodbye']\n",
        "ben_lst = ['ohe', 'kemon acho tumi', 'biday']\n",
        "len(ben_lst),len(en_lst)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YIAYcvYc7oA",
        "outputId": "34d21bf5-ca03-414d-eccc-45c0c0235ba0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('hello', '[start] ohe [end]'),\n",
              " ('how are you', '[start] kemon acho tumi [end]'),\n",
              " ('goodbye', '[start] biday [end]')]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_pairs=[]\n",
        "cnt=0\n",
        "for eng,ben in zip(en_lst,ben_lst):\n",
        "  if len(ben.split())>50:\n",
        "    continue\n",
        "  ben = \"[start] \" + ben + \" [end]\"\n",
        "  text_pairs.append((eng,ben))\n",
        "  cnt+=1\n",
        "  if cnt>=(2753069//3):breaka\n",
        "len(text_pairs)\n",
        "\n",
        "text_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SlcjVM6c914",
        "outputId": "e1254c6e-5e21-4b28-8c4e-0b9b438ea769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello, world! how are you doing? 12/3\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "# Define the characters to keep\n",
        "keep_chars = \".,?!| / -\"\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    # Convert to lowercase\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "\n",
        "    # Remove non-alphabetic characters, except for those in the keep_chars set and digits\n",
        "    stripped = tf.strings.regex_replace(lowercase, \"[^a-z0-9\" + re.escape(keep_chars) + \"]\", \"\")\n",
        "\n",
        "    return stripped\n",
        "\n",
        "# Example usage:\n",
        "strip_chars = \"!@#$%^&*()_-+=[]{}|;:',.<>?/\"\n",
        "input_text = tf.constant(\"Hello, World! How are you doing? 12/3\")\n",
        "processed_text = custom_standardization(input_text)\n",
        "\n",
        "print(processed_text.numpy().decode(\"utf-8\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "7EqsQIE7dkFM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0* len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9aUF2CLdnkt",
        "outputId": "9f9aebb9-d3a8-4cfe-b144-acddced9699d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('how are you', '[start] kemon acho tumi [end]'),\n",
              " ('hello', '[start] ohe [end]'),\n",
              " ('goodbye', '[start] biday [end]')]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "b3lRlyMJdD1P"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "sequence_length=5\n",
        "max_token=8\n",
        "source_vectorization = layers.TextVectorization(\n",
        "  max_tokens=max_token,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length,\n",
        "  standardize=custom_standardization\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "  max_tokens=max_token,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length + 1,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFv9nOHddUMt",
        "outputId": "5de99724-ad1b-4c3c-ab42-c146402338a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['how are you', 'hello', 'goodbye'],\n",
              " ['[start] kemon acho tumi [end]', '[start] ohe [end]', '[start] biday [end]'])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_ben_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "train_eng_texts,train_ben_texts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "STGacpi8dumE"
      },
      "outputs": [],
      "source": [
        "def createVector(pairs,vectorztn):\n",
        "  batch_size = 10000  # Adjust the batch size based on your available memory\n",
        "  num_batches = len(pairs) // batch_size + 1\n",
        "\n",
        "  for i in range(num_batches):\n",
        "      start_idx = i * batch_size\n",
        "      end_idx = min((i + 1) * batch_size, len(pairs))\n",
        "      batch_texts = pairs[start_idx:end_idx]\n",
        "      vectorztn.adapt(batch_texts)\n",
        "      # target_vectorization.adapt(batch_ben_texts)  # Uncomment if needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['how are you', 'hello', 'goodbye']"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_eng_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "FBQqkCCldwTF"
      },
      "outputs": [],
      "source": [
        "createVector(train_eng_texts,source_vectorization)\n",
        "createVector(train_ben_texts,target_vectorization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u80B2yIZd30N",
        "outputId": "fd31cc8e-9105-4a5c-9ed6-c6aa3e3cebd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary: ['', '[UNK]', 'you', 'how', 'hello', 'goodbye', 'are']\n",
            "Max Tokens: 8\n",
            "Output Sequence Length: 5\n",
            "Output Mode: int\n",
            "Standardization: <function custom_standardization at 0x0000021B673085E0>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocabulary = source_vectorization.get_vocabulary()\n",
        "\n",
        "# Access other relevant information\n",
        "max_tokens = source_vectorization.get_config()['max_tokens']\n",
        "output_sequence_length = source_vectorization.get_config()['output_sequence_length']\n",
        "output_mode = source_vectorization.get_config()['output_mode']\n",
        "standardization = source_vectorization.get_config()['standardize']\n",
        "\n",
        "# Print or inspect the information\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"Max Tokens:\", max_tokens)\n",
        "print(\"Output Sequence Length:\", output_sequence_length)\n",
        "print(\"Output Mode:\", output_mode)\n",
        "print(\"Standardization:\", standardization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "m8EhvqVpeEi8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assuming source_vectorization and target_vectorization are TextVectorization layers\n",
        "\n",
        "batch_size =1\n",
        "\n",
        "def format_dataset(eng, ben):\n",
        "    print(ben)\n",
        "    eng = source_vectorization(eng)\n",
        "    ben = target_vectorization(ben)\n",
        "    print(ben )\n",
        "    # Ensure ben has the expected shape before slicing\n",
        "    # ben = tf.reshape(ben, [tf.shape(ben)[0], -1])  # Reshape to 2D\n",
        "    return {\"eng\": eng, \"ben\": ben[:, :-1]}, ben[:, 1:]\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ben_texts = zip(*pairs)\n",
        "    print(eng_texts)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ben_texts = list(ben_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ben_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('how are you', '[start] kemon acho tumi [end]'),\n",
              " ('hello', '[start] ohe [end]'),\n",
              " ('goodbye', '[start] biday [end]')]"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "fr7fqMGKeIkD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('how are you', 'hello', 'goodbye')\n",
            "Tensor(\"args_1:0\", shape=(None,), dtype=string)\n",
            "Tensor(\"text_vectorization_3/RaggedToTensor/RaggedTensorToTensor:0\", shape=(None, 6), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "train_ds = make_dataset(train_pairs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFf42oNNeMaT"
      },
      "source": [
        "## ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iVWrtje7eOPX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "camnoY_peQzk"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7cSe3DDjeULb"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyuX7OtreXNG"
      },
      "source": [
        "## THY MODEL COMETH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvswsLiSei7l",
        "outputId": "49a9e5c0-b623-4391-c2ab-da4e847ef9a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "C7Wv2FkEeY4-",
        "outputId": "c675a525-ce89-4fbd-d078-b57bd61f0b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From s:\\Thesis_Work\\tff\\Lib\\site-packages\\keras\\src\\layers\\normalization\\layer_normalization.py:328: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 7\n",
        "dense_dim = 5\n",
        "num_heads = 3 \n",
        "vocab_size=max_token\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"eng\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"ben\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, 5)(decoder_inputs)\n",
        "x = TransformerDecoder(5, 1, 1)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.35)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KAG50rM_enrv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From s:\\Thesis_Work\\tff\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " eng (InputLayer)            [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " ben (InputLayer)            [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 7)              91        ['eng[0][0]']                 \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 5)              65        ['ben[0][0]']                 \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 7)              768       ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 5)              306       ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 5)              0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 8)              48        ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1278 (4.99 KB)\n",
            "Trainable params: 1278 (4.99 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<CacheDataset element_spec=({'eng': TensorSpec(shape=(None, 5), dtype=tf.int64, name=None), 'ben': TensorSpec(shape=(None, 5), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 5), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrQot0egepjo",
        "outputId": "92d6464d-256c-4bdf-e133-2cc515e05298"
      },
      "outputs": [],
      "source": [
        "transformer.fit(train_ds, epochs=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh2rSGQCe_cb",
        "outputId": "f50a95e8-859c-4e79-ff93-2d2855ea9a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'how are you', 'goodbye']\n",
            "-\n",
            "hello\n",
            "[start] ohe end\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 5\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        # print(predictions)\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"end\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_pairs=train_pairs\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "print(test_eng_texts)\n",
        "\n",
        "# >>>>>>>>>>>>>>>>>\n",
        "input_sentence = random.choice(test_eng_texts)\n",
        "print(\"-\")\n",
        "print(input_sentence)\n",
        "print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result Array 1: [[2. 0. 0. 0. 0. 0.]]\n",
            "Result Array 2: [[2. 3. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def pad_array(input_array, desired_length):\n",
        "    if len(input_array.shape) == 1:\n",
        "        input_array = np.array([input_array])\n",
        "    \n",
        "    current_length = input_array.shape[1]\n",
        "    \n",
        "    if current_length >= desired_length:\n",
        "        return input_array\n",
        "    \n",
        "    # Calculate the number of zeros to add\n",
        "    zeros_to_add = desired_length - current_length\n",
        "    \n",
        "    # Create a padding array with zeros\n",
        "    padding = np.zeros((input_array.shape[0], zeros_to_add))\n",
        "    \n",
        "    # Concatenate the padding array to the input_array\n",
        "    result_array = np.concatenate((input_array, padding), axis=1)\n",
        "    \n",
        "    return result_array\n",
        "\n",
        "# Example usage:\n",
        "array1 = np.array([[2.]])\n",
        "array2 = np.array([2., 3])\n",
        "\n",
        "result_array1 = pad_array(array1, 6)\n",
        "result_array2 = pad_array(array2, 6)\n",
        "\n",
        "print(\"Result Array 1:\", result_array1)\n",
        "print(\"Result Array 2:\", result_array2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8wLU9dtLgOeY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'how are you', 'goodbye']\n",
            "hello\n",
            ">>>>>>>>>> [2.]\n",
            "[[array([2., 6., 4., 0., 0., 0.]), 16.171515756868757], [array([2., 5., 5., 0., 0., 0.]), 18.846020584227517], [array([2., 5., 3., 0., 0., 0.]), 20.474569075973704], [array([2., 6., 4., 0., 0., 4.]), 23.64329182705842], [array([2., 6., 1., 0., 5., 3.]), 25.040064440574497], [array([2., 5., 5., 0., 0., 4.]), 26.077192548196763], [array([2., 6., 4., 0., 3., 0.]), 27.426995853427798], [array([2., 6., 1., 0., 5., 0.]), 27.75573524227366], [array([2., 6., 1., 0., 3., 0.]), 27.888044032617472], [array([2., 5., 3., 0., 0., 4.]), 28.0618867832236], [array([2., 5., 5., 0., 3., 0.]), 30.362288037664257], [array([2., 5., 5., 3., 0., 0.]), 30.362640558509156], [array([2., 5., 3., 0., 3., 0.]), 32.176250254153274], [array([2., 6., 1., 4., 5., 3.]), 32.271445982158184], [array([2., 6., 4., 4., 5., 3.]), 32.37278962135315], [array([2., 6., 4., 4., 5., 0.]), 33.69346046447754], [array([2., 6., 4., 4., 3., 0.]), 34.39391063223593], [array([2., 6., 1., 4., 5., 0.]), 34.82415556907654], [array([2., 6., 1., 4., 3., 0.]), 34.873843817505985], [array([2., 6., 4., 0., 3., 4.]), 35.17821544210892], [array([2., 6., 1., 0., 3., 5.]), 35.19243916263804], [array([2., 5., 3., 4., 5., 3.]), 37.5826163738966], [array([2., 5., 5., 3., 0., 4.]), 37.694186299457215], [array([2., 5., 5., 0., 3., 4.]), 38.17667734785937], [array([2., 5., 3., 4., 3., 0.]), 39.36108038993552], [array([2., 5., 3., 4., 5., 0.]), 39.69661045074463], [array([2., 5., 5., 3., 5., 3.]), 40.232476860284805], [array([2., 5., 3., 0., 3., 4.]), 40.272580383112654], [array([2., 5., 5., 3., 5., 0.]), 41.83382439613342], [array([2., 6., 4., 4., 3., 4.]), 41.985472679138184], [array([2., 6., 1., 4., 3., 5.]), 42.23942470550537], [array([2., 5., 3., 4., 3., 4.]), 47.279706954956055]]\n",
            "2.0 start\n",
            "6.0 kemon\n",
            "4.0 tumi\n",
            "0.0 \n",
            "0.0 \n",
            "0.0 \n"
          ]
        }
      ],
      "source": [
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "\n",
        "\n",
        "\n",
        "def TBeamSearch(inputs,decoder_input,decoder_model,outputs_maxlen,beam_size=2):\n",
        "  \n",
        "  \n",
        "  max_len=outputs_maxlen\n",
        "\n",
        "  decoded_sentence = np.zeros((1, 1))\n",
        "  decoded_sentence[0, 0] = 2\n",
        " \n",
        "\n",
        "  sequences = [[decoded_sentence, 0.0]]\n",
        "\n",
        "\n",
        "  decoded_sentence = \"[start]\"\n",
        "  tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "\n",
        " \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  ln=sequences[0][0][0]\n",
        "  print('>>>>>>>>>>',ln)\n",
        " \n",
        "  ii=0\n",
        "  while ii < max_len:\n",
        "      all_candidates = []\n",
        "\n",
        "\n",
        "\n",
        "      for i in range(len(sequences)):\n",
        "          seq, score = sequences[i]\n",
        "          \n",
        "          tokenized_target_sentence=pad_array(seq,max_len)\n",
        "\n",
        " \n",
        "\n",
        "          probs = transformer(\n",
        "            [inputs, tokenized_target_sentence])\n",
        "          word_preds = np.argsort(probs)[0][ii][-beam_size:]\n",
        "\n",
        "\n",
        "          for w in word_preds:\n",
        "              candidate_seq = [\n",
        "                  # np.concatenate([seq, np.array([w])], axis=1),\n",
        "                  np.append(seq, w),\n",
        "                      score - np.log(probs[-1][-1][w])]\n",
        "                      \n",
        "              new_states = w\n",
        "          \n",
        "              all_candidates.append((candidate_seq, new_states))\n",
        "      ii+=1\n",
        "      all_candidates.sort(key=lambda tup: tup[0][1])\n",
        "      sequences = [tup[0] for tup in all_candidates]\n",
        "      states = [tup[1] for tup in all_candidates]\n",
        "\n",
        "\n",
        "\n",
        "      ln=(sequences[0][0][0])\n",
        "  min_array = min(sequences, key=lambda x: x[1])\n",
        "  return min_array[0],sequences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "print(test_eng_texts)\n",
        "\n",
        "input_sentence = random.choice(test_eng_texts)\n",
        "print(input_sentence)\n",
        "tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "# input >>>>>>>>>>>>>>>>>>\n",
        "\n",
        "\n",
        "decoded_sentence = \"[start]\"\n",
        "\n",
        "tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "\n",
        "tokenized_target_sentence\n",
        "\n",
        "x,y=TBeamSearch(tokenized_input_sentence , tokenized_target_sentence , transformer , 5)\n",
        "print(y)\n",
        "\n",
        "for xx in x:\n",
        "  print(xx,spa_index_lookup[xx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2., 6., 7., 0., 1., 4.])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
