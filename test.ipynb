{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'you', 'world', 'how', 'hello', 'goodbye', 'are']\n",
      "['', '[UNK]', 'thak', 'mundo', 'kemon', 'hola', 'acos']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15\n",
    "sequence_length = 4\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "eng_texts = [\"Hello world.\", \"How are you?\", \"Goodbye!\"]\n",
    "spa_texts = [\"Hola mundo.\", \"kemon acos??\", \"thak\"]\n",
    "\n",
    "\n",
    "\n",
    "source_vectorization.adapt(eng_texts)\n",
    "target_vectorization.adapt(spa_texts)\n",
    "print(source_vectorization.get_vocabulary())\n",
    "print(target_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch:\n",
      "[b'Hello world. ||| Hola mundo.' b'How are you? ||| kemon acos??']\n",
      "Processed Batch:\n",
      "[b'Goodbye! ||| thak']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Dummy data\n",
    "eng_texts = [\"Hello world.\", \"How are you?\", \"Goodbye!\"]\n",
    "spa_texts = [\"Hola mundo.\", \"kemon acos??\", \"thak\"]\n",
    "\n",
    "# Create a TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "\n",
    "# Batch the data\n",
    "batch_size = 2\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Dummy map function\n",
    "def format_dataset(eng_batch, spa_batch):\n",
    "    # This is a dummy map function; you would replace it with your actual processing logic\n",
    "    # For illustration purposes, we concatenate the English and Spanish sentences\n",
    "    return tf.strings.join([eng_batch, spa_batch], separator=\" ||| \")\n",
    "\n",
    "# Apply the map function\n",
    "dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "\n",
    "# Display the processed dataset\n",
    "for batch in dataset.take(-1):\n",
    "    print(\"Processed Batch:\")\n",
    "    print(batch.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Batch:\n",
      "({'english': <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[5, 3, 0, 0]], dtype=int64)>, 'spanish': <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[5, 3, 0, 0]], dtype=int64)>}, <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[3, 0, 0, 0]], dtype=int64)>)\n",
      "English: [[5 3 0 0]]\n",
      "Spanish Input: [[5 3 0 0]]\n",
      "Spanish Output: [[3 0 0 0]]\n",
      "\n",
      ">>>>>>>>\n",
      "Processed Batch:\n",
      "({'english': <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[4, 7, 2, 0]], dtype=int64)>, 'spanish': <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[4, 6, 0, 0]], dtype=int64)>}, <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[6, 0, 0, 0]], dtype=int64)>)\n",
      "English: [[4 7 2 0]]\n",
      "Spanish Input: [[4 6 0 0]]\n",
      "Spanish Output: [[6 0 0 0]]\n",
      "\n",
      ">>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Format the dataset using your format_dataset function\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "# Apply the map function\n",
    "dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "\n",
    "# Display the processed dataset\n",
    "for batch in dataset.take(2):\n",
    "    print(\"Processed Batch:\")\n",
    "    print(batch)\n",
    "    print(\"English:\", batch[0][\"english\"].numpy())\n",
    "    print(\"Spanish Input:\", batch[0][\"spanish\"].numpy())\n",
    "    print(\"Spanish Output:\", batch[1].numpy())\n",
    "    print(\"\\n>>>>>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original English: Hello world.\n",
      "Processed English: [5 3 0 0] 4\n",
      "Original Spanish: Hola mundo.\n",
      "Processed Spanish: [5 3 0 0 0] 5\n",
      "\n",
      "\n",
      "Original English: How are you?\n",
      "Processed English: [4 7 2 0] 4\n",
      "Original Spanish: ¿Cómo estás?\n",
      "Processed Spanish: [1 1 0 0 0] 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'train_english_texts' and 'train_spanish_texts' are your training data\n",
    "train_pairs = [(\"Hello world.\", \"Hola mundo.\"), (\"How are you?\", \"¿Cómo estás?\")]\n",
    "\n",
    "# Example usage of the format_dataset function\n",
    "for eng, spa in train_pairs:\n",
    "    eng_processed = source_vectorization(eng)\n",
    "    spa_processed = target_vectorization(spa)\n",
    "\n",
    "    print(\"Original English:\", eng)\n",
    "    print(\"Processed English:\", eng_processed.numpy(),len(eng_processed.numpy()))\n",
    "    print(\"Original Spanish:\", spa)\n",
    "    print(\"Processed Spanish:\", spa_processed.numpy(),len(spa_processed.numpy()))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "eng_sentence1 = \"Hello world.\"\n",
    "spa_sentence1 = \"Hola mundo.\"\n",
    "\n",
    "eng_sentence2 = \"How are you?\"\n",
    "spa_sentence2 = \"¿Cómo estás?\"\n",
    "\n",
    "# Process the sentences using format_dataset\n",
    "example1 = format_dataset(eng_sentence1, spa_sentence1)\n",
    "example2 = format_dataset(eng_sentence2, spa_sentence2)\n",
    "\n",
    "# Display the processed examples\n",
    "print(\"Example 1:\")\n",
    "print(\"Input (English):\", example1[0][\"english\"].numpy())\n",
    "print(\"Output (Spanish, shifted):\", example1[1].numpy())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Example 2:\")\n",
    "print(\"Input (English):\", example2[0][\"english\"].numpy())\n",
    "print(\"Output (Spanish, shifted):\", example2[1].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Crossentropy Loss (one-hot encoded): [0.35667494 1.60943791]\n",
      "Sparse Categorical Crossentropy Loss (integers): [0.35667494 1.60943791]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.losses import categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Example data\n",
    "num_classes = 3\n",
    "num_samples = 2\n",
    "\n",
    "# True class labels (assuming a classification task with three classes)\n",
    "true_labels = np.array([1, 2])  # Class indices (not one-hot encoded)\n",
    "\n",
    "# One-hot encoding of true labels\n",
    "one_hot_labels = to_categorical(true_labels, num_classes=num_classes)\n",
    "\n",
    "# Predictions (example output from a neural network)\n",
    "predictions = np.array([[0.2, 0.7, 0.1], [0.6, 0.2, 0.2]])\n",
    "\n",
    "# Using categorical_crossentropy (requires one-hot encoded targets)\n",
    "loss_categorical = categorical_crossentropy(one_hot_labels, predictions)\n",
    "print(\"Categorical Crossentropy Loss (one-hot encoded):\", loss_categorical.numpy())\n",
    "\n",
    "# Using sparse_categorical_crossentropy (integers as targets)\n",
    "loss_sparse_categorical = sparse_categorical_crossentropy(true_labels, predictions)\n",
    "print(\"Sparse Categorical Crossentropy Loss (integers):\", loss_sparse_categorical.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
